{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import copy\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_concept_pairs(spark, phenotype_concepts_input, phenotype_concepts_out, save_pair_concepts=True, header=\"true\", delimiter=\"\\t\"):\n",
    "    #Load the data into the spark dataframe\n",
    "    phenotypeDF_1 = spark.read \\\n",
    "        .option(\"header\", header) \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .csv(phenotype_concepts_input)\n",
    "        #.withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"phenotype_name\").orderBy(desc(\"source_concept_id\"))))\n",
    "    \n",
    "    #Load the data into the spark dataframe again for performing self join later \n",
    "    phenotypeDF_2 = phenotypeDF_1.rdd.toDF(phenotypeDF_1.schema)\n",
    "    \n",
    "    #Alias the columns for the pairwise concepts within the same phenotype definitions\n",
    "    columns = [phenotypeDF_1[\"phenotype_name\"],\n",
    "           phenotypeDF_1[\"source_concept_id\"].alias(\"source_concept_id_1\"),\n",
    "           phenotypeDF_1[\"standard_concept_id\"].alias(\"standard_concept_id_1\"),\n",
    "           phenotypeDF_1[\"source_name\"].alias(\"source_name_1\"),\n",
    "           phenotypeDF_1[\"source_vocabulary\"].alias(\"source_vocabulary_1\"),\n",
    "           phenotypeDF_1[\"source_domain\"].alias(\"source_domain_1\"),\n",
    "           phenotypeDF_1[\"standard_name\"].alias(\"standard_name_1\"),\n",
    "           phenotypeDF_1[\"standard_vocabulary\"].alias(\"standard_vocabulary_1\"),\n",
    "           phenotypeDF_1[\"standard_domain\"].alias(\"standard_domain_1\"),\n",
    "           phenotypeDF_2[\"source_concept_id\"].alias(\"source_concept_id_2\"),\n",
    "           phenotypeDF_2[\"standard_concept_id\"].alias(\"standard_concept_id_2\"),\n",
    "           phenotypeDF_2[\"source_name\"].alias(\"source_name_2\"),\n",
    "           phenotypeDF_2[\"source_vocabulary\"].alias(\"source_vocabulary_2\"),\n",
    "           phenotypeDF_2[\"source_domain\"].alias(\"source_domain_2\"),\n",
    "           phenotypeDF_2[\"standard_name\"].alias(\"standard_name_2\"),\n",
    "           phenotypeDF_2[\"standard_vocabulary\"].alias(\"standard_vocabulary_2\"),\n",
    "           phenotypeDF_2[\"standard_domain\"].alias(\"standard_domain_2\"),\n",
    "          ]\n",
    "    #Create all combinations of concept pairs within the same phenotype definitions. \n",
    "    #Self join the phenotype dataset where rows are NOT the same\n",
    "    pair_concepts = phenotypeDF_1.join(phenotypeDF_2, phenotypeDF_1[\"phenotype_name\"] == phenotypeDF_2[\"phenotype_name\"]) \\\n",
    "        .where((phenotypeDF_1[\"standard_concept_id\"] != phenotypeDF_2[\"standard_concept_id\"])\n",
    "           | (phenotypeDF_1[\"source_concept_id\"] != phenotypeDF_2[\"source_concept_id\"])) \\\n",
    "        .select(columns).orderBy(phenotypeDF_1[\"phenotype_name\"])\n",
    "    \n",
    "    #determine if we need to save the dataframe to the disk\n",
    "    if save_pair_concepts:\n",
    "        #Save the paired concepts to a file\n",
    "        pair_concepts.write \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format(\"csv\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(\"phenotype_paired_concepts\")\n",
    "    \n",
    "    #extract all phenotype related concepts\n",
    "    phenotype_concepts = phenotypeDF_1.select(col(\"standard_concept_id\").alias(\"concept_id\")) \\\n",
    "        .union(phenotypeDF_1.select(col(\"source_concept_id\").alias(\"concept_id\"))) \\\n",
    "        .distinct();\n",
    "    \n",
    "    return (pair_concepts, phenotype_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cdm_tables(spark, property_ini_file_path):\n",
    "    \n",
    "    #Parse the properties\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(property_ini_file_path)\n",
    "    properties = config.defaults()\n",
    "    base_url = properties[\"base_url\"]\n",
    "    \n",
    "    #Load visit_occurrence\n",
    "    visit_occurrence = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.visit_occurrence\", properties=properties)\n",
    "\n",
    "    #Load condition_occurrence\n",
    "    condition_occurrence = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.condition_occurrence\", properties=properties)\n",
    "\n",
    "    #Load drug_exposure\n",
    "    drug_exposure = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.drug_exposure\", properties=properties)\n",
    "\n",
    "    #Load procedure_occurrence\n",
    "    procedure_occurrence = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.procedure_occurrence\", properties=properties)\n",
    "\n",
    "    #Load measurement\n",
    "    measurement = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.measurement\", properties=properties)\n",
    "\n",
    "    #Load observation\n",
    "    observation = spark.read \\\n",
    "        .jdbc(base_url, \"dbo.observation\", properties=properties)\n",
    "        \n",
    "    return (visit_occurrence, condition_occurrence, drug_exposure, procedure_occurrence, measurement, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_domain_to_visit(domain_tables, visit_occurrence):\n",
    "    \n",
    "    joined_domain_tables = []\n",
    "    \n",
    "    for domain_table in domain_tables:\n",
    "        #extract the domain concept_id from the table fields. E.g. condition_concept_id from condition_occurrence\n",
    "        concept_id_field = [f for f in domain_table.schema.fieldNames() if \"concept_id\" in f][0]\n",
    "        #extract the name of the table\n",
    "        table_domain_field = concept_id_field.replace(\"_concept_id\", \"\")\n",
    "        #limit the domain records to those which have a visit_occurrence_id\n",
    "        joined_domain_table = domain_table \\\n",
    "            .join(v, domain_table[\"visit_occurrence_id\"] == v[\"visit_occurrence_id\"])\n",
    "        #standardize the output columns\n",
    "        joined_domain_tables.append(\n",
    "            joined_domain_table \\\n",
    "                .select(domain_table[\"person_id\"], \n",
    "                    domain_table[\"visit_occurrence_id\"], \n",
    "                    domain_table[concept_id_field].alias(\"standard_concept_id\"), \n",
    "                    lit(table_domain_field).alias(\"domain\"))\n",
    "        )\n",
    "        \n",
    "    return joined_domain_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_visit_concept(condition, \n",
    "                               drug, \n",
    "                               procedure, \n",
    "                               measurement, \n",
    "                               observation, \n",
    "                               patient_visit_concept_output,\n",
    "                               concept_occurrence_output):\n",
    "    \n",
    "    #Union person_id, visit_occurrence_id, and concept_id from all domains\n",
    "    patient_visit_concept = condition \\\n",
    "        .union(drug) \\\n",
    "        .union(procedure) \\\n",
    "        .union(measurement) \\\n",
    "        .union(observation) \\\n",
    "        .distinct() \\\n",
    "        .orderBy(\"person_id\", \"visit_occurrence_id\") \\\n",
    "        .where(col(\"standard_concept_id\") != 0)\n",
    "    \n",
    "    #Save the patient visit concept data \n",
    "    patient_visit_concept.write.option(\"header\", \"true\") \\\n",
    "        .format(\"csv\").mode(\"overwrite\") \\\n",
    "        .save(patient_visit_concept_output)\n",
    "    \n",
    "    #Create the concept occurrence matrix\n",
    "    concept_occurrence_matrix = patient_visit_concept \\\n",
    "        .groupBy(\"standard_concept_id\").count()\n",
    "        \n",
    "    #Save the patient visit concept data \n",
    "    concept_occurrence_matrix.write.option(\"header\", \"true\") \\\n",
    "        .format(\"csv\").mode(\"overwrite\") \\\n",
    "        .save(concept_occurrence_output)\n",
    "    \n",
    "    return (patient_visit_concept, concept_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(patient_visit_concept, concept_occurrence_matrix, cooccurrence_matrix_output, phenotype_concepts=None):\n",
    "     \n",
    "    #If phenotype_concepts is specified, the records that contain phenotype concepts are kept\n",
    "    if phenotype_concepts != None:\n",
    "        patient_visit_concept = patient_visit_concept \\\n",
    "        .join(phenotype_concepts, patient_visit_concept[\"standard_concept_id\"] == phenotype_concepts[\"concept_id\"]) \\\n",
    "        .select(patient_visit_concept[\"person_id\"], \n",
    "                patient_visit_concept[\"visit_occurrence_id\"], \n",
    "                patient_visit_concept[\"standard_concept_id\"], \n",
    "                patient_visit_concept[\"domain\"])\n",
    "    \n",
    "    #Add ranks to dataframe to avoid the symetric pairs generated by the self-join operation\n",
    "    patient_visit_concept = patient_visit_concept.withColumn(\"rank\", \n",
    "            dense_rank().over(Window.partitionBy(\"person_id\", \"visit_occurrence_id\").orderBy(desc(\"standard_concept_id\"))))\n",
    "    \n",
    "    #Make two copies of the patient_visit_concept dataframe for self-join\n",
    "    pvc_1 = patient_visit_concept.rdd.toDF(patient_visit_concept.schema)\n",
    "    pvc_2 = patient_visit_concept.rdd.toDF(patient_visit_concept.schema)\n",
    "    \n",
    "    #Create the cooccurrence matrix via a self-join where the concept_ids are NOT the same\n",
    "    cooccurrence_matrix = pvc_1 \\\n",
    "        .join(pvc_2, (pvc_1[\"person_id\"] == pvc_2[\"person_id\"])\n",
    "              & (pvc_1[\"visit_occurrence_id\"] == pvc_2[\"visit_occurrence_id\"])) \\\n",
    "        .where(pvc_1[\"standard_concept_id\"] != pvc_2[\"standard_concept_id\"]) \\\n",
    "        .select(pvc_1[\"person_id\"].alias(\"person_id\"),\n",
    "                pvc_1[\"standard_concept_id\"].alias(\"standard_concept_id_1\"), \n",
    "                pvc_2[\"standard_concept_id\"].alias(\"standard_concept_id_2\")) \\\n",
    "        .groupBy(\"standard_concept_id_1\", \"standard_concept_id_2\").count()\n",
    "    \n",
    "    cooccurrence_matrix = cooccurrence_matrix \\\n",
    "        .join(concept_occurrence_matrix, \n",
    "                        cooccurrence_matrix[\"standard_concept_id_1\"] == concept_occurrence_matrix[\"standard_concept_id\"]) \\\n",
    "        .select(cooccurrence_matrix[\"standard_concept_id_1\"],\n",
    "                cooccurrence_matrix[\"standard_concept_id_2\"],\n",
    "                cooccurrence_matrix[\"count\"],\n",
    "                concept_occurrence_matrix[\"count\"].alias(\"standard_concept_id_1_count\")\n",
    "               ) \\\n",
    "        .join(concept_occurrence_matrix, \n",
    "                        cooccurrence_matrix[\"standard_concept_id_2\"] == concept_occurrence_matrix[\"standard_concept_id\"]) \\\n",
    "        .select(cooccurrence_matrix[\"standard_concept_id_1\"],\n",
    "                cooccurrence_matrix[\"standard_concept_id_2\"],\n",
    "                cooccurrence_matrix[\"count\"],\n",
    "                col(\"standard_concept_id_1_count\"),\n",
    "                concept_occurrence_matrix[\"count\"].alias(\"standard_concept_id_2_count\")\n",
    "               ) \\\n",
    "        .withColumn(\"normalized_count\", col(\"count\") / (col(\"standard_concept_id_1_count\") + col(\"standard_concept_id_2_count\")))\n",
    "    \n",
    "    #Save the cooccurrence matrix\n",
    "    cooccurrence_matrix.write.option(\"header\", \"true\") \\\n",
    "        .format(\"csv\").mode(\"overwrite\").save(cooccurrence_matrix_output)\n",
    "    \n",
    "    return cooccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"Phenotype Cooccurrence\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "\n",
    "    pair_concepts, phenotype_concepts = \\\n",
    "        collect_concept_pairs(spark, \"phenotype_WG_concept_ids_from_eMERGE.tsv\", \"phenotype_paired_concepts\")\n",
    "\n",
    "    v, c, d, p, m, o = load_cdm_tables(spark, \"omop_database_properties.ini\")\n",
    "\n",
    "    c_filtered, d_filtered, p_filtered, m_filtered, o_filtered \\\n",
    "        = join_domain_to_visit([c, d, p, m, o], v)\n",
    "        \n",
    "    patient_visit_concept, concept_occurrence = create_patient_visit_concept(c_filtered, \n",
    "                                                                              d_filtered, \n",
    "                                                                              p_filtered, \n",
    "                                                                              m_filtered, \n",
    "                                                                              o_filtered, \n",
    "                                                                              \"patient_visit_concept\", \n",
    "                                                                              \"concept_occurrence\")\n",
    "        \n",
    "    #cooccurrence_matrix = create_cooccurrence_matrix(patient_visit_concept, \n",
    "    #                                                 concept_occurrence,\n",
    "    #                                                 \"cooccurrence_matrix\", \n",
    "    #                                                 phenotype_concepts)\n",
    "\n",
    "    cooccurrence_matrix_full = create_cooccurrence_matrix(patient_visit_concept, \n",
    "                                                     concept_occurrence,\n",
    "                                                     \"cooccurrence_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.jdbc.\n: java.lang.ClassNotFoundException: net.sourceforge.jtds.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:238)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0038d452285b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cdm_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"omop_database_properties.ini\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0916b07f8a72>\u001b[0m in \u001b[0;36mload_cdm_tables\u001b[0;34m(spark, property_ini_file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Load visit_occurrence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvisit_occurrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dbo.visit_occurrence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Load condition_occurrence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mjpredicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.jdbc.\n: java.lang.ClassNotFoundException: net.sourceforge.jtds.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:238)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "v, c, d, p, m, o = load_cdm_tables(spark, \"omop_database_properties.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
